## **Upaya Mencapai Kedalaman**

Setelah AlexNet, para peneliti mulai membuat jaringan konvolusional yang lebih dalam. Mereka menemukan bahwa semakin banyak lapisan yang ditambahkan, semakin baik akurasi klasifikasinya. Ada beberapa alasan yang menjelaskan hal ini:  

#### **Kemampuan Representasi yang Lebih Baik**  
Satu lapisan dalam jaringan hanya mampu melakukan operasi linear, sehingga sulit untuk menangani pola yang kompleks, tidak peduli berapa banyak parameternya. Namun, jika setiap lapisan menggunakan fungsi aktivasi nonlinear seperti sigmoid atau ReLU, menumpuk beberapa lapisan akan menghasilkan kombinasi nonlinier yang lebih banyak. Ini membuat jaringan lebih mampu mengenali pola yang rumit, seperti membedakan gambar kucing dan anjing.  

#### **Kemampuan Generalisasi yang Lebih Baik**  
Jika hanya ada satu lapisan dengan banyak parameter, jaringan cenderung hanya "menghafal" contoh data yang diberikan tanpa benar-benar memahami polanya. Hal ini membuat jaringan kurang mampu mengenali data baru. Sebaliknya, dengan menambah lebih banyak lapisan, jaringan akan belajar mengenali fitur secara bertahap. Misalnya, lapisan awal mengenali elemen dasar seperti bulu dan kumis, lalu lapisan berikutnya menyusun fitur-fitur tersebut menjadi bentuk kepala kucing, hingga akhirnya mengenali seluruh kucing. Dengan cara ini, jaringan menjadi lebih baik dalam mengklasifikasikan gambar baru.  

#### **Melihat Area yang Lebih Luas dengan Cara Lebih Efisien**  
Jika kepala kucing dalam gambar memiliki ukuran 128x128 piksel, maka jaringan konvolusional dengan satu lapisan akan membutuhkan filter berukuran sama besar untuk mengenalinya, yang akan sangat membebani sistem. Sebaliknya, dengan menggunakan beberapa lapisan yang lebih dalam, jaringan bisa memakai filter kecil berukuran 3x3 atau 5x5, tetapi tetap bisa menangkap informasi dari area yang luas. Dengan kedalaman yang cukup, jaringan tetap bisa memahami keseluruhan gambar tanpa harus menggunakan filter besar yang tidak efisien.  

Agar jaringan yang lebih dalam tetap efisien dan tidak memiliki terlalu banyak parameter, para peneliti mulai mengembangkan jenis lapisan konvolusional yang lebih hemat. Mari kita lihat bagaimana cara mereka melakukannya.

### **Faktorisasi Filter**  

Mana yang lebih baik: satu filter konvolusi 5x5 atau dua filter 3x3 yang diterapkan secara berurutan?  

Keduanya memiliki area reseptif yang sama, yaitu 5x5 (lihat Gambar 3-18). Meskipun operasi matematisnya tidak persis sama, efeknya cenderung mirip. Perbedaannya terletak pada jumlah parameter yang dipelajari oleh jaringan.  

- Satu filter 5x5 memiliki **5 × 5 = 25** bobot yang dapat dipelajari.  
- Dua filter 3x3 yang diterapkan secara berurutan memiliki total **2 × 3 × 3 = 18** bobot yang dapat dipelajari.  

Karena dua filter 3x3 membutuhkan lebih sedikit parameter, metode ini lebih **efisien** dibandingkan menggunakan satu filter 5x5.

# CODENYA HILANG!

Rata-rata menghilangkan banyak informasi posisi yang ada di saluran. Itu mungkin atau mungkin bukan hal yang baik tergantung pada aplikasi. Filter konvolusional mendeteksi hal-hal yang mereka miliki telah dilatih untuk mendeteksi di lokasi tertentu. Jika jaringan adalah classi Misalnya, kucing versus anjing, data lokasi (misalnya, "kucing kumis" yang terdeteksi pada posisi x, y di saluran) mungkin tidak berguna di kepala klasifikasi. Satu-satunya hal yang menarik adalah Sinyal "anjing terdeteksi di mana saja" versus "kucing terdeteksi di mana saja" tanda. Namun, untuk aplikasi lain, pooling rata-rata global lapisan mungkin bukan pilihan terbaik. Misalnya, dalam deteksi objek atau kasus penggunaan penghitungan objek, lokasi objek yang terdeteksi adalah Pengumpulan Rata-rata Penting dan Global Tidak Boleh Digunakan
